{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3744449-67ca-4bf8-9caf-92169282c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "pdf_dir = \"../../data/pdf/\"\n",
    "txt_path = \"../../data/txt/pdf_contents.txt\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(txt_path), exist_ok=True)\n",
    "\n",
    "# Get a list of all PDF files in the directory\n",
    "pdf_files = glob.glob(os.path.join(pdf_dir, \"*.pdf\"))\n",
    "\n",
    "# Initialize a variable to hold the combined content\n",
    "combined_content = \"\"\n",
    "\n",
    "# Loop through all PDF files\n",
    "for pdf_file in pdf_files:\n",
    "    # Open the PDF document\n",
    "    pdf_document = fitz.open(pdf_file)\n",
    "    \n",
    "    # Extract text from each page of the PDF\n",
    "    content = \"\"\n",
    "    for page_number in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_number]\n",
    "        content += page.get_text()\n",
    "    \n",
    "    # Close the PDF document\n",
    "    pdf_document.close()\n",
    "    \n",
    "    # Add a header for each PDF (optional)\n",
    "    combined_content += f\"--- Contents of {os.path.basename(pdf_file)} ---\\n\"\n",
    "    combined_content += content.strip() + \"\\n\\n\"\n",
    "\n",
    "# Write the combined content to the text file\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "    txt_file.write(combined_content)\n",
    "\n",
    "print(f\"Combined PDF contents saved to {txt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855562e-70e6-41f2-886a-465dac62c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Define additional stop words\n",
    "ADDITIONAL_STOPWORDS = {\"bank\", \"fdic\"}\n",
    "\n",
    "def clean_text(file_path, output_path):\n",
    "    # Ensure NLTK resources are downloaded\n",
    "    try:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    except LookupError:\n",
    "        import nltk\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('punkt')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    stop_words.update(ADDITIONAL_STOPWORDS)\n",
    "\n",
    "    # Prepare punctuation to exclude periods\n",
    "    punctuation_to_remove = string.punctuation.replace('.', '')\n",
    "\n",
    "    # Initialize the stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Remove numbers from the line\n",
    "        line = re.sub(r'\\d+', '', line)\n",
    "        # Tokenize line\n",
    "        tokens = word_tokenize(line)\n",
    "        # Filter and process tokens\n",
    "        tokens = [\n",
    "            stemmer.stem(token.lower()) for token in tokens\n",
    "            if token.lower() not in stop_words\n",
    "            and token not in punctuation_to_remove\n",
    "            and len(token) > 1\n",
    "        ]\n",
    "        # Reconstruct cleaned sentence\n",
    "        cleaned_line = ' '.join(tokens)\n",
    "        cleaned_lines.append(cleaned_line)\n",
    "\n",
    "    # Save cleaned content to a new file\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write('\\n'.join(cleaned_lines))\n",
    "\n",
    "    print(f\"Cleaned text saved to {output_path}\")\n",
    "\n",
    "# File paths\n",
    "input_file_path = '../../data/txt/pdf_contents.txt'\n",
    "output_file_path = '../../data/txt/cleaned_contents.txt'\n",
    "\n",
    "# Run the function\n",
    "clean_text(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f30d1-af75-4b84-993d-3801c5506a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove punctuation\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    return list(ngrams(words, n))\n",
    "\n",
    "def visualize_ngrams(ngrams_freq, title, top_n=10):\n",
    "    top_ngrams = ngrams_freq.most_common(top_n)\n",
    "    labels, counts = zip(*top_ngrams)\n",
    "    labels = [' '.join(label) for label in labels]  # Join n-grams with spaces\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(labels, counts, color='skyblue')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "# File path\n",
    "file_path = '../../data/txt/cleaned_contents.txt'  # Replace with your file path\n",
    "\n",
    "# Main processing\n",
    "text = preprocess_text(read_file(file_path))\n",
    "\n",
    "# N-gram extraction\n",
    "for n in range(1, 5):  # Unigrams to Qualgrams\n",
    "    ngrams_list = get_ngrams(text, n)\n",
    "    ngrams_freq = Counter(ngrams_list)\n",
    "    visualize_ngrams(ngrams_freq, f'Top {n}-grams', top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1569472-e0e4-4400-8ae0-464258c9e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from IPython.display import display\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Ensure NLTK dependencies are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee9b45a-4783-4bd6-821f-48c832742c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_lda(input_file_path):\n",
    "    \"\"\"\n",
    "    Preprocess the cleaned text for LDA modeling by tokenizing and creating a document-term matrix.\n",
    "    \"\"\"\n",
    "    # Read the cleaned text file\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    processed_lines = [word_tokenize(line.strip()) for line in lines if line.strip()]\n",
    "\n",
    "    return processed_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a75be9-0c33-4388-b8b0-34647c6b40f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to the cleaned text\n",
    "input_file_path = '../../data/txt/cleaned_contents.txt'\n",
    "\n",
    "# Preprocess text\n",
    "processed_lines = preprocess_for_lda(input_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d8dfe-4764-408b-ad92-0aed17ab7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(processed_lines)\n",
    "\n",
    "# Create a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_lines]\n",
    "\n",
    "# Perform LDA topic modeling\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    num_topics=4,\n",
    "    id2word=dictionary,\n",
    "    passes=10,\n",
    "    random_state=493\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7f48e-4401-4c7d-a8c3-bb2361f2609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=7)  # num_words shows top words in each topic\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea906da0-ff17-4354-92b2-f7f63ca4bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_topic(bow):\n",
    "    topic_probs = lda_model.get_document_topics(bow)\n",
    "    return max(topic_probs, key=lambda x: x[1])[0] if topic_probs else None\n",
    "\n",
    "topic_numbers = [get_dominant_topic(bow) for bow in corpus]\n",
    "\n",
    "# Step 6: Create the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Topic Number': topic_numbers,\n",
    "    'Sentence': processed_lines\n",
    "})\n",
    "df.to_csv('../../data/txt/lda_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe43b0-5b9b-4d38-9b1c-f5ee1a2c948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_scores(processed_lines, start=2, limit=6):\n",
    "    \"\"\"Compute coherence scores for various numbers of topics.\"\"\"\n",
    "    coherence_scores = []\n",
    "    for num_topics in range(start, limit + 1):\n",
    "        lda_model = gensim.models.LdaModel(\n",
    "            corpus,\n",
    "            num_topics=num_topics,\n",
    "            id2word=dictionary,\n",
    "            passes=10,\n",
    "            random_state=493\n",
    "        )\n",
    "        coherence_model = CoherenceModel(model=lda_model, texts=processed_lines, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        coherence_scores.append((num_topics, coherence_score))\n",
    "        print(f\"Number of Topics: {num_topics}, Coherence Score: {coherence_score}\")\n",
    "\n",
    "    return coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bbcea2-a9ed-4cc0-b26a-d10c6a361cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coherence scores\n",
    "coherence_scores = compute_coherence_scores(processed_lines, start=2, limit=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8c1d3-2e2a-41fd-8e37-91ff5850999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coherence_scores(coherence_scores):\n",
    "    \"\"\"Plot coherence scores for different numbers of topics.\"\"\"\n",
    "    num_topics = [score[0] for score in coherence_scores]\n",
    "    scores = [score[1] for score in coherence_scores]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(num_topics, scores, marker='o', linestyle='--', color='b')\n",
    "    plt.title('Coherence Scores by Number of Topics')\n",
    "    plt.xlabel('Number of Topics')\n",
    "    plt.ylabel('Coherence Score')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c732a-2ac7-4cd2-b616-1aa76dce784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coherence scores\n",
    "plot_coherence_scores(coherence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a256fd8-3905-4162-b086-deca13f27152",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "gensimvis.prepare(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e55512-cba9-4e1f-9f7b-3b8fe7ece93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_ngrams(text, n):\n",
    "    \"\"\"\n",
    "    Generate n-grams from text.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    return list(ngrams(words, n))\n",
    "\n",
    "def visualize_ngrams(ngrams_freq, title, top_n=10):\n",
    "    \"\"\"\n",
    "    Visualize the top n-grams in a horizontal bar chart.\n",
    "    \"\"\"\n",
    "    top_ngrams = ngrams_freq.most_common(top_n)\n",
    "    if not top_ngrams:  # Skip empty n-grams\n",
    "        print(f\"No n-grams to display for {title}\")\n",
    "        return\n",
    "    \n",
    "    labels, counts = zip(*top_ngrams)\n",
    "    labels = [' '.join(label) for label in labels]  # Join n-grams with spaces\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(labels, counts, color='skyblue')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee7dff-3b20-4544-b7f2-f3ec5304e862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load topics DataFrame\n",
    "topics_df = pd.read_csv('../../data/txt/lda_topics.csv')  # Ensure the path is correct\n",
    "\n",
    "# Visualize n-grams for each topic\n",
    "for _, row in topics_df.iterrows():\n",
    "    topic_number = row['Topic Number']\n",
    "    content = row['Sentence']\n",
    "    \n",
    "    print(f\"Visualizing n-grams for Topic {topic_number}...\")\n",
    "    \n",
    "    for n in range(1, 5):  # Unigrams, Bigrams, Trigrams\n",
    "        ngrams_list = get_sub_ngrams(content, n)\n",
    "        ngrams_freq = Counter(ngrams_list)\n",
    "        visualize_ngrams(ngrams_freq, f'Topic {topic_number} - Top {n}-grams', top_n=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
